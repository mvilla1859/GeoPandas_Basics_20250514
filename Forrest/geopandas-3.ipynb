{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Geospatial Data with GeoPandas: Supported Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data from this notebook comes from the [NYC open data portal](https://opendata.cityofnewyork.us/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import geopandas as gpd\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many file types, same read function: ```gpd.read_file()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designated pedestrian zones in NYC\n",
    "shapefile = \"../../geopandas_101_DATA/Forrest/data/Pedestrian Zone Shapefile (Tabular)_20241220/geo_export_a7bb075a-41dc-445f-8244-8430e90a8dde.shp\"\n",
    "shapefile_gdf = gpd.read_file(shapefile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shapefile_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC unofficial neighborhoods\n",
    "geojson_fp = \"https://raw.githubusercontent.com/HodgesWardElliott/custom-nyc-neighborhoods/refs/heads/master/custom-pedia-cities-nyc-Mar2018.geojson\"\n",
    "geojson_gdf = gpd.read_file(geojson_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geojson_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geo Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# World rivers\n",
    "# gpkg_fp = \"https://ngageoint.github.io/GeoPackage/examples/rivers.gpkg\"\n",
    "gpkg_fp = \"../../geopandas_101_DATA/Soma/data/rivers.gpkg\"\n",
    "gpkg_gdf = gpd.read_file(gpkg_fp)\n",
    "gpkg_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat Geo Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# US counties\n",
    "# fgb_fp = \"https://github.com/flatgeobuf/flatgeobuf/raw/refs/heads/master/test/data/UScounties.fgb\"\n",
    "fgb_fp = \"../../geopandas_101_DATA/Soma/data/UScounties.fgb\"\n",
    "fgb_gdf = gpd.read_file(fgb_fp)\n",
    "fgb_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SQL tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though you can do everything in Python using GeoPandas, there are some really good reasons to combine it with the use of SQL or a tool like DuckDB instead:\n",
    "\n",
    "\n",
    "Efficiency\n",
    "\n",
    "* **DuckDB can be faster** for certain operations — particularly **spatial joins and aggregations**.\n",
    "* SQL queries are executed within a **high-performance, in-memory engine** (DuckDB), which can outperform pandas/GeoPandas when handling large datasets.\n",
    "\n",
    "\n",
    "Simplified data prep\n",
    "\n",
    "* SQL can **condense data cleaning and transformation** (e.g., joins, filters, type casting) into **fewer lines** of declarative logic.\n",
    "* For example, filtering by date and doing a spatial intersection can be written as one SQL query instead of multiple pandas/GeoPandas steps.\n",
    "\n",
    "\n",
    "Avoiding memory overload\n",
    "\n",
    "* With GeoPandas, you must **load entire datasets into memory**.\n",
    "* DuckDB allows you to **process data on demand** — which can be more scalable, especially with large files (like zipped CSVs or remote Parquet files).\n",
    "\n",
    "\n",
    "Combining diverse file types\n",
    "\n",
    "* SQL (via DuckDB) can read **remote or local files**, such as Parquet, CSV, or even GeoPackages, and **query them together**.\n",
    "* This is harder to do cleanly in pure Python.\n",
    "\n",
    "**In short**: It's not about replacing GeoPandas — it’s about giving you more tools for the job; extending the ecosystem. You might use DuckDB/SQL to gain speed, memory efficiency, and cleaner preprocessing — especially for **larger datasets** or **more complex queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post GIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import duckdb\n",
    "\n",
    "\n",
    "# Define PostGIS connection parameters\n",
    "host = \"your_host\"           # e.g., \"localhost\" or your database IP\n",
    "database = \"your_database\"   # Database name\n",
    "user = \"your_user\"           # Username\n",
    "password = \"your_password\"   # Password\n",
    "port = \"5432\"                # Default PostgreSQL port\n",
    "\n",
    "# SQLAlchemy connection string for PostGIS\n",
    "postgis_connection = f\"postgresql://{user}:{password}@{host}:{port}/{database}\"\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(postgis_connection)\n",
    "\n",
    "# Define SQL query to read spatial data\n",
    "# Replace 'your_table_name' with the actual table containing spatial data\n",
    "query = \"SELECT * FROM your_table_name\"\n",
    "\n",
    "# Load the PostGIS table into a GeoDataFrame\n",
    "postgis_gdf = gpd.read_postgis(query, con=engine, geom_col=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "postgis_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DuckDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pipenv install duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "\n",
    "# Initialize a DuckDB connection\n",
    "con = duckdb.connect()\n",
    "con.query('INSTALL spatial')\n",
    "con.query('LOAD spatial')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying remote CSVs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below queries a series of six CSV files that are containd in a zip file, which is hosted in AWS. Those CSVs contain all the bike rides of the NYC Citi Bike system for the for the month of June 2024. Each CSV has the following columns:\n",
    "\n",
    "*ride_id, rideable_type, started_at, ended_at, start_station_name, start_station_id, end_station_name, end_station_id, start_lat, start_lng, end_lat, end_lng, member_casual*\n",
    "\n",
    "Instead of downloading the whole zip file, unzipping it, etc., we can query the CSVs directly and only download the data we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DuckDB SQL query\n",
    "duckdb_query = \"\"\"\n",
    "SELECT *, ST_AsText(ST_Point(column09, column08)) as geometry\n",
    "FROM read_csv('https://s3.amazonaws.com/tripdata/202406-citibike-tripdata.zip', ignore_errors=true)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute DuckDB query and convert to a Pandas DataFrame\n",
    "duckdb_df = con.query(duckdb_query).to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(duckdb_df.shape)\n",
    "duckdb_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_gdf = gpd.GeoDataFrame(\n",
    "    duckdb_df,\n",
    "    geometry=gpd.GeoSeries.from_wkt(duckdb_df[\"geometry\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_gdf.columns = [\n",
    "'ride_id', 'rideable_type', 'started_at', 'ended_at', \n",
    "    'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id', \n",
    "    'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual',\n",
    "    'geometry'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_gdf.head(1000).explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying remote GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server = \"https://raw.githubusercontent.com/HodgesWardElliott/custom-nyc-neighborhoods/refs/heads/master/custom-pedia-cities-nyc-Mar2018.geojson\"\n",
    "\n",
    "# Instead of calling the whole dataset using gdp.read_file(), like this:\n",
    "# neighborhoods = gpd.read_file()\n",
    "\n",
    "# we instead run a limited query:xqd\n",
    "nbh_query = f\"\"\"\n",
    "SELECT *\n",
    "FROM ST_Read('{server}')\n",
    "limit 10\n",
    "\"\"\"\n",
    "print(nbh_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb_df = con.query(nbh_query).to_df()\n",
    "duckdb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run a simultaneous online query of both the CSVs and the GeoJSON files above.\n",
    "\n",
    "This SQL query is designed to perform a spatial analysis that links bicycle trip data to New York City neighborhoods. It illustrates how to integrate geospatial data with real-world activity data using DuckDB, a high-performance analytical database, and ST\\_\\* spatial functions.\n",
    "\n",
    "The query begins by selecting two data sources: a GeoJSON file containing custom-defined NYC neighborhoods (each with a name and polygon geometry), and a zipped CSV file of Citi Bike trips for June 2024. It then performs a spatial join between these two datasets using the `ST_Intersects` function. This spatial predicate checks whether a bike trip’s starting point (reconstructed from columns 9 and 8 in the trip data—likely longitude and latitude, respectively) falls within the geometry of any NYC neighborhood.\n",
    "\n",
    "From there, it filters the results to keep only those trips that occurred on June 15th, 2024. The query groups the results by neighborhood name and geometry, and counts the number of trips that fall into each neighborhood on that day.\n",
    "\n",
    "Finally, the result is a table where each row corresponds to a neighborhood, and includes:\n",
    "\n",
    "* the number of Citi Bike trips that started within its boundaries on June 15, 2024,\n",
    "* the neighborhood name,\n",
    "* and its geometry in well-known text (WKT) format, ready for mapping or further analysis.\n",
    "\n",
    "This analysis is useful for understanding spatial patterns in bike usage—such as identifying which neighborhoods had the most activity on a given day. It demonstrates how spatial joins, data filtering, and aggregation can combine to produce valuable geographic insights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_query = \"\"\"\n",
    "SELECT \n",
    "    count(b.column00) as count,           -- Count the number of Citi Bike trips that matched the spatial condition\n",
    "    n.neighborhood,                       -- Select the neighborhood name from the neighborhoods GeoJSON\n",
    "    ST_AsText(n.geom) as geom            -- Convert the neighborhood geometry to Well-Known Text (WKT) format for easy readability/output\n",
    "FROM \n",
    "    ST_Read(\n",
    "        'https://raw.githubusercontent.com/HodgesWardElliott/custom-nyc-neighborhoods/refs/heads/master/custom-pedia-cities-nyc-Mar2018.geojson'\n",
    "    ) n                                  -- Load the NYC neighborhood geometries from a remote GeoJSON file; alias this table as 'n'\n",
    "JOIN \n",
    "    read_csv(\n",
    "        'https://s3.amazonaws.com/tripdata/202406-citibike-tripdata.zip', \n",
    "        ignore_errors=true\n",
    "    ) b                                  -- Load the zipped Citi Bike trip data CSV from AWS S3; alias this table as 'b'\n",
    "ON \n",
    "    ST_Intersects(n.geom, ST_Point(column09, column08)) \n",
    "                                         -- Perform a spatial join: check if the starting point of a bike trip \n",
    "                                         -- (constructed from column09 = longitude, column08 = latitude) intersects the neighborhood geometry\n",
    "WHERE \n",
    "    CAST(column02 AS DATE) = DATE '2024-06-15'\n",
    "                                         -- Filter to include only bike trips that took place on June 15, 2024\n",
    "GROUP BY \n",
    "    n.neighborhood, n.geom               -- Group the results by neighborhood name and geometry\n",
    "\"\"\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_df = con.query(nbh_query).to_df()\n",
    "nbh_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_gdf = gpd.GeoDataFrame(\n",
    "    nbh_df,\n",
    "    geometry=gpd.GeoSeries.from_wkt(nbh_df[\"geom\"]),\n",
    "    crs=\"EPSG:4326\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet files with ```gpd.read_parquet()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_parquet('../../geopandas_101_DATA/Forrest/data/es_cn.parquet')\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head(100).explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somebody he mentioned took the City Bike zip data and transformed int a parquet file [here](https://source.coop/repositories/zluo43/citibike/description). He suggested it as a good tool to practice how to use the ```gdp.read_parquet()``` function. I think his point is that the files themselves are already the results of queries. Each parquet file file in there is the result of a query, so by downoading them it is equivalent to having alrady run the queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
